"""
Research Agent API

This module provides the public API for the research agent.
"""

import logging
import re
from typing import Dict, List, Optional

from .models import ResearchResult, SearchResult
from .search_engine import ResearchAgent

# Configure logging
logger = logging.getLogger(__name__)

def search_deep(topic: str, depth: str = "medium") -> ResearchResult:
    """
    Public API: Perform a deep search on a topic with the default ResearchAgent.
    
    Args:
        topic: The topic to research
        depth: Search depth - "low", "medium", or "high"
        
    Returns:
        ResearchResult containing comprehensive information about the topic
    """
    agent = ResearchAgent()
    return agent.search_deep(topic, depth)

def search_basic(topic: str, num_results: int = 5) -> List[Dict[str, str]]:
    """
    Perform a basic search on a topic and return simplified results
    
    This function guarantees that exactly `num_results` results will be returned.
    If fewer results are available from the web search, synthetic results will
    be generated to reach the requested number. This ensures consistent behavior
    for downstream functions that expect a specific number of results.
    
    Args:
        topic: The topic to search for
        num_results: Number of results to return (guaranteed)
        
    Returns:
        List of exactly `num_results` simplified search results as dictionaries.
        Each dictionary contains "title", "source", and "summary" keys.
    """
    logger.info(f"ğŸ” åŸºæœ¬æ¤œç´¢ã‚’å®Ÿè¡Œ: {topic} (çµæœæ•°: {num_results})")
    
    try:
        # Create a research agent
        agent = ResearchAgent()
        
        # Perform a simplified search
        search_results = agent.search_web(topic, num_results=num_results, depth="low")
        
        # Convert to simplified dictionary format and enrich with real titles if needed
        simplified_results = []
        
        for result in search_results:
            # Clean the URL
            url = result.url.strip()
            
            # Remove trailing parentheses and markup that might be part of the URL
            url = re.sub(r'\)$', '', url)
            url = re.sub(r'\]$', '', url)
            
            # If URL contains markdown link format [title](url), extract just the URL
            md_url_match = re.search(r'\((https?://[^\s\)]+)\)', url)
            if md_url_match:
                url = md_url_match.group(1)
                
            # Extract real title if it's in markdown format
            title = result.title.strip()
            markdown_title_pattern = r'\[(.*?)\]'
            markdown_title_match = re.search(markdown_title_pattern, title)
            if markdown_title_match:
                title = markdown_title_match.group(1).strip()
            else:
                # If not in markdown, clean up any other artifacts
                title = re.sub(r'\*\*|\*|[-â€¢]|\bURL:|\bURL\b|\[|\]|\(|\)|`|__', '', title).strip()
            
            # Check if the title is just a domain name
            if title.endswith('.jp') or title.endswith('.com') or title.endswith('.org') or title.endswith('.net'):
                # Try to get a better title from the URL domain parts
                domain_parts = url.split('://')[-1].split('/')[0].split('.')
                if len(domain_parts) >= 2:
                    site_name = domain_parts[-2].capitalize()
                    title = f"{site_name} - {topic}ã«é–¢ã™ã‚‹æƒ…å ±"
            
            # Clean up the summary
            summary = result.snippet.strip() if result.snippet else ""
            
            # If summary contains markdown links, clean them
            summary = re.sub(r'\[(.*?)\]\((https?://[^\s\)]+)\)', r'\1 (\2)', summary)
            
            # If summary is just a URL remnant or empty, provide a generic summary
            if not summary or summary.startswith('ook/') or summary.endswith('ai))"') or summary.endswith('ai))'):
                summary = f"{topic}ã«é–¢ã™ã‚‹æƒ…å ± - {title}"
            
            simplified_results.append({
                "title": title,
                "source": url,
                "summary": summary
            })
        
        # Check if we have fewer results than requested and add fallback results if needed
        if len(simplified_results) < num_results:
            logger.info(f"âš ï¸ æ¤œç´¢çµæœãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚è¿½åŠ çµæœã‚’ç”Ÿæˆã—ã¾ã™: è¦æ±‚æ•° {num_results}, å®Ÿéš›ã®çµæœ {len(simplified_results)}")
            # Add placeholder results to reach the requested number
            additional_needed = num_results - len(simplified_results)
            
            for i in range(additional_needed):
                simplified_results.append({
                    "title": f"{topic} - æƒ…å ± {len(simplified_results) + 1}",
                    "source": f"https://research.example.org/{topic}/{len(simplified_results) + 1}",
                    "summary": f"{topic}ã«é–¢ã™ã‚‹æƒ…å ±ã§ã™ã€‚å®Ÿéš›ã®æ¤œç´¢çµæœãŒååˆ†ã§ã¯ãªã‹ã£ãŸãŸã‚ã€è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè£œè¶³æƒ…å ±ã§ã™ã€‚"
                })
            logger.info(f"âœ… è¿½åŠ çµæœã‚’ç”Ÿæˆã—ã¾ã—ãŸ: åˆè¨ˆ {len(simplified_results)} ä»¶")
        
        logger.info(f"âœ… åŸºæœ¬æ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸ: {len(simplified_results)} ä»¶ã®çµæœ")
        return simplified_results
        
    except Exception as e:
        logger.error(f"âŒ åŸºæœ¬æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # Return a minimal result set to avoid breaking the pipeline
        dummy_results = []
        for i in range(num_results):
            dummy_results.append({
                "title": f"{topic} - æƒ…å ± {i+1}",
                "source": "https://example.com",
                "summary": f"{topic}ã«é–¢ã™ã‚‹æƒ…å ±ã§ã™ã€‚APIã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¤œç´¢ãŒã§ããªã„ãŸã‚ã€é™å®šçš„ãªæƒ…å ±ã®ã¿æä¾›ã—ã¦ã„ã¾ã™ã€‚"
            })
        return dummy_results 