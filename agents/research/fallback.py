"""
Fallback Search Mechanisms

This module provides fallback search mechanisms when online search APIs are unavailable.
"""

import logging
import hashlib
import random
import re
from typing import List, Dict, Any, Optional
from urllib.parse import quote_plus

import json
from .models import SearchResult

# Configure logging
logger = logging.getLogger(__name__)

def generate_synthetic_results(query: str, count: int = 5) -> List[SearchResult]:
    """
    Generate synthetic search results when real search fails.
    
    Args:
        query: The search query
        count: Number of results to generate
        
    Returns:
        List of synthetic SearchResult objects
    """
    logger.info(f"ğŸ§  ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã—ã¾ã™: {query} (ä»¶æ•°: {count})")
    
    # Clean the query for domain creation
    clean_query = re.sub(r'[^a-zA-Z0-9]', '', query.lower()) 
    if not clean_query:
        clean_query = "research"
    
    # Categories for synthetic resources
    categories = [
        {"type": "educational", "domains": ["example.edu", "academy.example.com", "knowledge.example.edu", "institute.example.org"]},
        {"type": "research", "domains": ["research.example.org", "science.example.net", "datasci.example.info", "lab.example.co.jp"]},
        {"type": "community", "domains": ["community.example.com", "forum.example.org", "wiki.example.info", "discussion.example.net"]}
    ]
    
    # Create templates for different types of resources
    templates = {
        "basics": f"{query}ã®åŸºç¤ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "advanced": f"ä¸Šç´š{query}æŠ€è¡“ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "applications": f"{query}ã®å¿œç”¨ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "algorithms": f"{query}ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "implementations": f"{query}ã®å®Ÿè£… - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "case_studies": f"{query}ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "future": f"{query}ã®æœªæ¥å±•æœ› - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "frameworks": f"{query}ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "tutorials": f"{query}ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
        "best_practices": f"{query}ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
    }
    
    # Create templates for snippets based on resource type
    snippet_templates = {
        "basics": f"{query}ã¯{query}ã®åŸºæœ¬æ¦‚å¿µã¨åŸç†ã‚’è§£èª¬ã—ã¦ã„ã¾ã™ã€‚åˆå¿ƒè€…å‘ã‘ã®å°å…¥ã‹ã‚‰å§‹ã¾ã‚Šã€{query}ã®æ­´å²ã€åŸºæœ¬ç†è«–ã€ä¸»è¦ãªæ§‹æˆè¦ç´ ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚...",
        "advanced": f"ä¸Šç´šè€…å‘ã‘ã®{query}æŠ€è¡“ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚æœ€æ–°ã®ç ”ç©¶æˆæœã€é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€å°‚é–€çš„ãªå¿œç”¨ä¾‹ã‚’å«ã¿ã¾ã™ã€‚...",
        "applications": f"{query}ã®å®Ÿéš›ã®å¿œç”¨ä¾‹ã¨ç”£æ¥­ã§ã®æ´»ç”¨æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚ã•ã¾ã–ã¾ãªåˆ†é‡ã§ã®å®Ÿç”¨ä¾‹ã‚„æˆåŠŸäº‹ä¾‹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚...",
        "algorithms": f"{query}ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯è¤‡æ•°ã‚ã‚Šã€ãã‚Œãã‚Œç‰¹æ€§ã‚„ç”¨é€”ãŒç•°ãªã‚Šã¾ã™ã€‚ä»£è¡¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä»•çµ„ã¿ã‚„å®Ÿè£…æ–¹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ã¾ã™ã€‚...",
        "implementations": f"{query}ã®å®Ÿè£…æ–¹æ³•ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ä¾‹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€ä¸€èˆ¬çš„ãªèª²é¡Œã¨è§£æ±ºç­–ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚...",
        "case_studies": f"{query}ã®å®Ÿéš›ã®æ´»ç”¨äº‹ä¾‹ã‚’è©³ç´°ã«åˆ†æã—ã¦ã„ã¾ã™ã€‚æˆåŠŸä¾‹ã¨å¤±æ•—ä¾‹ã®ä¸¡æ–¹ã‹ã‚‰å­¦ã¶ã¹ãæ•™è¨“ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚...",
        "future": f"{query}ã®ä»Šå¾Œã®ç™ºå±•æ–¹å‘ã‚„å°†æ¥æ€§ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ã„ã¾ã™ã€‚æœ€æ–°ã®ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„æœŸå¾…ã•ã‚Œã‚‹æŠ€è¡“é©æ–°ã«ã¤ã„ã¦ã‚‚è§¦ã‚Œã¦ã„ã¾ã™ã€‚...",
        "frameworks": f"{query}ã®ãŸã‚ã®ä¸»è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ„ãƒ¼ãƒ«ã‚’æ¯”è¼ƒè§£èª¬ã—ã¦ã„ã¾ã™ã€‚ãã‚Œãã‚Œã®ç‰¹å¾´ã‚„é©ã—ãŸä½¿ç”¨ã‚·ãƒ¼ãƒ³ãŒåˆ†æã•ã‚Œã¦ã„ã¾ã™ã€‚...",
        "tutorials": f"{query}ã‚’å®Ÿè·µçš„ã«å­¦ã¶ãŸã‚ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚åˆå¿ƒè€…ã‹ã‚‰ä¸Šç´šè€…ã¾ã§ã®ãƒ¬ãƒ™ãƒ«åˆ¥ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚...",
        "best_practices": f"{query}ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚ä¸€èˆ¬çš„ãªè½ã¨ã—ç©´ã‚„é¿ã‘ã‚‹ã¹ãèª¤ã‚Šã‚‚è§£èª¬ã•ã‚Œã¦ã„ã¾ã™ã€‚..."
    }
    
    results = []
    used_templates = []
    
    # Generate specified number of results
    for i in range(count):
        # Select a category
        category = random.choice(categories)
        domain = random.choice(category["domains"])
        
        # Select a template (avoid repeating if possible)
        available_templates = [k for k in templates.keys() if k not in used_templates]
        if not available_templates:
            available_templates = list(templates.keys())
        
        template_key = random.choice(available_templates)
        used_templates.append(template_key)
        if len(used_templates) > len(templates) // 2:
            used_templates.pop(0)  # Remove oldest to allow some repeats
            
        title = templates[template_key]
        
        # Create a URL using the domain
        path = f"{clean_query}/{i+1}"
        url = f"https://{domain}/{path}"
        
        # Create a snippet using the appropriate template
        snippet = snippet_templates[template_key]
        
        # Create and add the result
        result = SearchResult(
            url=url,
            title=title,
            snippet=snippet,
            source_type="synthetic"
        )
        results.append(result)
    
    logger.info(f"âœ… åˆæˆçµæœã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ: {len(results)} ä»¶")
    return results

def generate_offline_results(query: str, count: int = 5) -> List[SearchResult]:
    """
    Generate comprehensive results without using OpenAI API when quota is exceeded.
    
    Args:
        query: The search query
        count: Number of results to generate
        
    Returns:
        List of SearchResult objects
    """
    logger.info(f"ğŸ”Œ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™: {query}")
    
    # Just use the synthetic generator with a notice prefixed
    results = generate_synthetic_results(query, count)
    
    # Add a notice to each result
    for result in results:
        result.snippet = f"[ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ç”Ÿæˆ] " + result.snippet
        
    return results

def search_with_serpapi(query: str, api_key: str, num_results: int = 5) -> List[SearchResult]:
    """
    Search using SerpAPI as an alternative provider.
    
    Args:
        query: Search query
        api_key: SerpAPI key
        num_results: Number of results to return
        
    Returns:
        List of SearchResult objects
    """
    logger.info(f"ğŸ” SerpAPIã§æ¤œç´¢ä¸­: {query}")
    
    try:
        import requests
        
        # Prepare the request to SerpAPI
        params = {
            "engine": "google",
            "q": query,
            "api_key": api_key,
            "num": num_results,
            "hl": "ja"  # Japanese language results
        }
        
        response = requests.get(
            "https://serpapi.com/search", 
            params=params,
            timeout=10
        )
        
        if response.status_code != 200:
            logger.error(f"âŒ SerpAPI ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            return []
        
        data = response.json()
        
        # Extract organic results
        if "organic_results" not in data:
            logger.error("âŒ SerpAPI: æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return []
        
        results = []
        for item in data["organic_results"][:num_results]:
            url = item.get("link", "")
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            
            if not snippet and "rich_snippet" in item:
                # Try to get more details from rich snippet
                rich = item["rich_snippet"]
                if "top" in rich and "detected_extensions" in rich["top"]:
                    snippet = rich["top"]["detected_extensions"].get("description", "")
            
            result = SearchResult(
                url=url,
                title=title,
                snippet=snippet or f"{query}ã«é–¢ã™ã‚‹æƒ…å ±",
                source_type="serpapi"
            )
            results.append(result)
        
        logger.info(f"âœ… SerpAPIæ¤œç´¢å®Œäº†: {len(results)} ä»¶ã®çµæœ")
        return results
        
    except Exception as e:
        logger.error(f"âŒ SerpAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return [] 