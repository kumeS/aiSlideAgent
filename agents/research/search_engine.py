"""
Search Engine Module

This module provides the core search functionality for the research agent.
"""

import os
import json
import logging
import re
import time
import random
from typing import List, Dict, Any, Optional, Tuple
from dotenv import load_dotenv

from .models import SearchResult, ResearchResult
from .utils import check_api_quota, get_appropriate_model, clean_url
from .fallback import generate_synthetic_results, generate_offline_results, search_with_serpapi
from .credibility import evaluate_credibility, cluster_results
from .knowledge_gaps import extract_knowledge_gaps
from .summarization import generate_summary

from agents import client, DEFAULT_MODEL

# Configure logging
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
SERP_API_KEY = os.getenv("SERP_API_KEY")

class ResearchAgent:
    """Agent for performing multi-layer web research on topics."""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """Initialize the research agent with API keys and configuration."""
        self.serp_api_key = api_key or SERP_API_KEY
        self.model = model or DEFAULT_MODEL
        self.max_retries = 3
        self.fallback_mode = False
        
        # Use a less expensive model for search to avoid quota issues
        self.search_model = os.getenv("OPENAI_SEARCH_MODEL", "gpt-3.5-turbo-0125")
        
        # Alternative model for fallbacks
        self.fallback_model = os.getenv("OPENAI_FALLBACK_MODEL", "gpt-3.5-turbo-0125")
        
        # Check if models are available by testing API (without making a full search)
        self.api_quota_available = check_api_quota()
    
    def _get_appropriate_model(self, task_importance: str = "medium") -> str:
        """
        Get the appropriate model based on task importance and available budget.
        
        Args:
            task_importance: "high", "medium", or "low" indicating task criticality
            
        Returns:
            Model name to use for this task
        """
        return get_appropriate_model(
            self.api_quota_available, 
            self.fallback_model, 
            self.search_model, 
            self.model, 
            task_importance
        )
    
    def search_web(self, query: str, num_results: int = 10, depth: str = "medium") -> List[SearchResult]:
        """
        Perform a web search and return structured results.
        Primarily uses OpenAI's search-enabled models.
        
        Args:
            query: The search query
            num_results: Number of results to return
            depth: Search depth - "low", "medium", or "high"
        """
        # Check if we have API quota
        if not self.api_quota_available:
            logger.warning("âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€ä»£æ›¿æ¤œç´¢æ–¹æ³•ã‚’è©¦è¡Œã—ã¾ã™")
            
            # First try using an alternative search provider if available
            alternative_results = self._search_with_alternative_provider(query, num_results)
            if alternative_results:
                logger.info(f"âœ… ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰ {len(alternative_results)} ä»¶ã®çµæœã‚’å–å¾—")
                return alternative_results
            
            # If alternative search also fails, fall back to offline generation
            logger.warning("âš ï¸ ä»£æ›¿æ¤œç´¢ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç”Ÿæˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
            return generate_offline_results(query, num_results)
            
        # If we have quota, try OpenAI search
        return self._search_with_openai(query, num_results, depth)
    
    def _search_with_alternative_provider(self, query: str, num_results: int = 5) -> List[SearchResult]:
        """Use alternative search providers when OpenAI API is unavailable."""
        logger.info(f"ğŸ” ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ¤œç´¢ä¸­: {query}")
        
        try:
            # Try SerpAPI if key is available
            if self.serp_api_key:
                return search_with_serpapi(query, self.serp_api_key, num_results)
            
            # Add additional alternative search providers here
            # For example, you could implement:
            # - DuckDuckGo API (free)
            # - Bing API
            # - Custom web scraping solution
            
            # If no alternative providers are available
            logger.warning("âš ï¸ ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        except Exception as e:
            logger.error(f"âŒ ä»£æ›¿æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _search_with_openai(self, query: str, num_results: int = 10, depth: str = "medium") -> List[SearchResult]:
        """
        Use OpenAI's search-enabled model to perform web searches.
        
        This method guarantees that exactly `num_results` results will be returned.
        If fewer results are available from the web search, synthetic results 
        will be added to reach the requested number. This ensures that downstream
        components always receive the expected number of results.
        
        Args:
            query: The search query
            num_results: Number of results to return (guaranteed)
            depth: Search depth - "low", "medium", or "high"
            
        Returns:
            List of exactly `num_results` SearchResult objects, potentially including
            synthetic results if real search results are insufficient.
        """
        logger.info(f"ğŸ” æ¤œç´¢ä¸­: {query} (æ·±ã•: {depth})")
        
        try:
            # å‹å®‰å…¨æ€§ã®ç¢ºä¿: depth ãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
            if not isinstance(depth, str):
                logger.warning(f"âš ï¸ æ¤œç´¢æ·±åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¾ã™: {depth} â†’ 'medium'")
                depth = "medium"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«è¨­å®š
                
            # Map depth string to search_context_size
            search_depth = {
                "low": "low",
                "medium": "medium",
                "high": "high",
                # Add support for alternative values that might be passed
                "small": "low",
                "large": "high",
                "shallow": "low",
                "deep": "high"
            }.get(depth.lower(), "medium")
            
            # Log the actual search depth being used
            if depth.lower() != search_depth:
                logger.info(f"ğŸ” æ¤œç´¢æ·±åº¦ã®ãƒãƒƒãƒ”ãƒ³ã‚°: '{depth}' â†’ '{search_depth}'")
            
            # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
            # æ¤œç´¢å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã ã‘ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            search_capable_models = ["gpt-4o-search-preview", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-2024-04-09"]
            
            # å¯èƒ½ãªé™ã‚Šã€æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            search_model = None
            
            # ã¾ãšç’°å¢ƒå¤‰æ•°ã§æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
            for model_name in search_capable_models:
                if os.getenv(f"OPENAI_USE_{model_name.replace('-', '_').upper()}", "false").lower() == "true":
                    search_model = model_name
                    logger.info(f"ğŸ” ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€Œ{search_model}ã€ã‚’ä½¿ç”¨")
                    break
            
            # ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šãŒãªã‘ã‚Œã°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            if not search_model:
                # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
                search_model = "gpt-4o-search-preview"  # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥æŒ‡å®š
                logger.info(f"ğŸ” æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€Œ{search_model}ã€ã‚’ä½¿ç”¨")
            
            # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ä¸å¯ã¨æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä»£æ›¿æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if os.getenv("OPENAI_DISABLE_SEARCH", "false").lower() == "true":
                logger.warning(f"âš ï¸ æ¤œç´¢æ©Ÿèƒ½ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»£æ›¿æ–¹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return generate_synthetic_results(query, num_results)
            
            # Construct a prompt that will return structured search results
            system_prompt = """
            You are a search engine assistant that helps find and summarize information from the web.
            Search the web for the query, and return results in this EXACT format for each result:

            ---
            Title: [ACTUAL PAGE TITLE, not just domain name]
            URL: [FULL URL]
            Summary: [2-3 sentence summary of the actual page content, not just meta description]
            ---

            Provide real page titles - not just domain names. 
            Summaries should be informative and based on actual content.
            Do not use markdown formatting in your response.
            """
            
            # Retry with slight variations if needed
            max_search_retries = 2
            search_retry_count = 0
            response = None
            
            while search_retry_count <= max_search_retries and response is None:
                try:
                    # Make the API request with web search enabled
                    query_to_use = query
                    
                    # On retry, add more context to help the search
                    if search_retry_count == 1:
                        query_to_use = f"detailed information about {query}"
                    elif search_retry_count == 2:
                        query_to_use = f"{query} guide tutorial information"
                    
                    response = client.chat.completions.create(
                        model=search_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": f"Search for: {query_to_use}"}
                        ],
                        max_tokens=1500,
                        web_search_options={
                            "search_context_size": search_depth,  # Use the mapped depth
                            "user_location": {
                                "type": "approximate",
                                "approximate": {
                                    "country": "JP",  # Default to Japan
                                },
                            },
                        }
                    )
                except Exception as search_error:
                    search_retry_count += 1
                    # Format error message to be more user-friendly
                    error_str = str(search_error)
                    
                    # Extract and simplify common error messages
                    if "invalid_value" in error_str and "web_search_options.search_context_size" in error_str:
                        simplified_error = "æ¤œç´¢æ·±åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™ã€‚æœ‰åŠ¹ãªå€¤: 'low', 'medium', 'high'"
                    elif "not supported with this model" in error_str and "Web search" in error_str:
                        simplified_error = f"ãƒ¢ãƒ‡ãƒ« '{search_model}' ã¯ã‚¦ã‚§ãƒ–æ¤œç´¢ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™ã€‚"
                    elif "insufficient_quota" in error_str or "billing" in error_str.lower():
                        simplified_error = "APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä»£æ›¿æ¤œç´¢ã‚’è©¦è¡Œã—ã¾ã™ã€‚"
                    elif "Rate limit" in error_str:
                        simplified_error = "ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ¶é™: çŸ­æ™‚é–“ã«å¤šãã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒè¡Œã‚ã‚Œã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¾ã™ã€‚"
                    elif "auth" in error_str.lower() and "error" in error_str.lower():
                        simplified_error = "èªè¨¼ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    else:
                        # Keep original error but limit verbosity
                        shortened_error = error_str[:100] + "..." if len(error_str) > 100 else error_str
                        simplified_error = f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {shortened_error}"
                    
                    logger.warning(f"æ¤œç´¢è©¦è¡Œ {search_retry_count}: {simplified_error}")
                    
                    if search_retry_count > max_search_retries:
                        # ã™ã¹ã¦ã®æ¤œç´¢ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                        logger.error(f"æ¤œç´¢å¤±æ•—: 3å›è©¦è¡Œã—ã¾ã—ãŸãŒæˆåŠŸã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_str}")
                        logger.info("ä»£æ›¿æƒ…å ±ç”Ÿæˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
                        raise  # Re-raise if we've exhausted our retries
                    time.sleep(2)  # Brief pause before retrying
            
            if response is None:
                raise Exception("All search attempts failed")
            
            # Extract search results from the response
            content = response.choices[0].message.content.strip()
            
            # Log the raw response for debugging
            logger.debug(f"Raw search response: {content[:1000]}...")
            
            # Parse the results using the clean format we specified
            search_results = []
            result_blocks = content.split("---")
            
            for block in result_blocks:
                if not block.strip():
                    continue
                    
                title_match = re.search(r'Title:\s*(.*?)(?:\n|$)', block, re.IGNORECASE)
                url_match = re.search(r'URL:\s*(https?://\S+)(?:\n|$)', block, re.IGNORECASE)
                summary_match = re.search(r'Summary:\s*([\s\S]*?)(?=\n\s*$|\Z)', block, re.IGNORECASE)
                
                if title_match and url_match:
                    title = title_match.group(1).strip()
                    url = url_match.group(1).strip()
                    
                    # Clean up the URL - remove trailing parentheses and markup
                    url = re.sub(r'\)$', '', url)
                    url = re.sub(r'\]$', '', url)
                    
                    # If URL contains markdown link format [title](url), extract just the URL
                    md_url_match = re.search(r'\((https?://[^\s\)]+)\)', url)
                    if md_url_match:
                        url = md_url_match.group(1)
                    
                    # Clean up the title - remove markdown formatting
                    title = re.sub(r'\*\*|\*|__|\[|\]|\(|\)|`', '', title)
                    
                    summary = summary_match.group(1).strip() if summary_match else "No summary available"
                    
                    search_result = SearchResult(
                        url=url,
                        title=title,
                        snippet=summary,
                        source_type="ai_search"
                    )
                    search_results.append(search_result)
            
            # If no results from blocks, try older patterns
            if not search_results:
                # Try to match Title, URL, Summary pattern
                title_url_summary_pattern = r'(?:^|\n)(?:\d+\.\s*)?Title:\s*(.*?)(?:\n|\r\n)URL:\s*(https?://\S+)(?:\n|\r\n)Summary:\s*((?:.|\n)*?)(?=(?:^|\n)(?:\d+\.\s*)?Title:|$)'
                matches = re.findall(title_url_summary_pattern, content, re.MULTILINE | re.DOTALL)
                
                if matches:
                    for match in matches:
                        if len(match) >= 3:
                            title = match[0].strip()
                            url = match[1].strip()
                            summary = match[2].strip()
                            
                            search_result = SearchResult(
                                url=url,
                                title=title,
                                snippet=summary,
                                source_type="ai_search"
                            )
                            search_results.append(search_result)
            
            # If still no results, try JSON pattern
            if not search_results:
                json_pattern = r'\{[^{}]*"title"[^{}]*"url"[^{}]*"snippet"[^{}]*\}'
                json_objects = re.findall(json_pattern, content, re.DOTALL)
                
                if json_objects:
                    for json_str in json_objects:
                        try:
                            cleaned_json = re.sub(r'([{,]\s*)(\w+)(\s*:)', r'\1"\2"\3', json_str)
                            cleaned_json = cleaned_json.replace("'", '"')
                            result_data = json.loads(cleaned_json)
                            
                            if "title" in result_data and "url" in result_data and "snippet" in result_data:
                                search_result = SearchResult(
                                    url=result_data["url"],
                                    title=result_data["title"],
                                    snippet=result_data["snippet"],
                                    source_type="ai_search"
                                )
                                search_results.append(search_result)
                        except Exception as e:
                            logger.warning(f"Failed to parse JSON object: {e}")
            
            # If still no results, try numbered list pattern
            if not search_results:
                result_pattern = r'(?:\d+[\.\)]\s*|â€¢\s*|\*\s*)(.*?)\n\s*(?:URL|Link):\s*(https?://\S+)\s*(?:\n|$).*?(?:Snippet|Summary|Description):\s*(.*?)(?:\n\s*(?:\d+[\.\)]|â€¢|\*)|$)'
                matches = re.findall(result_pattern, content, re.DOTALL)
                
                for match in matches:
                    if len(match) >= 3:
                        title = match[0].strip()
                        url = match[1].strip()
                        snippet = match[2].strip()
                        
                        search_result = SearchResult(
                            url=url,
                            title=title,
                            snippet=snippet,
                            source_type="ai_search"
                        )
                        search_results.append(search_result)
            
            # If still no results, extract URLs
            if not search_results:
                url_pattern = r'((?:https?://)[^\s\)]+)'
                urls = re.findall(url_pattern, content)
                
                for url in urls:
                    # Find context around this URL
                    url_pos = content.find(url)
                    context_start = max(0, content.rfind('\n', 0, url_pos))
                    context_end = content.find('\n', url_pos + len(url))
                    if context_end == -1:
                        context_end = len(content)
                    
                    context = content[context_start:context_end].strip()
                    
                    # Extract title - assume it's at the beginning of the context
                    title_end = min(100, len(context))
                    title = context[:title_end].strip()
                    if len(title) > 50:
                        title = title[:50] + "..."
                    
                    # Use remaining context as snippet
                    snippet = context[len(title):].strip()
                    if not snippet:
                        snippet = f"Information about {query}"
                    
                    search_result = SearchResult(
                        url=url,
                        title=title,
                        snippet=snippet,
                        source_type="ai_search"
                    )
                    search_results.append(search_result)
            
            # If we still have no results, generate synthetic results
            if not search_results:
                logger.warning(f"âš ï¸ ã‚¦ã‚§ãƒ–æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆæˆçµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                synthetic_results = generate_synthetic_results(query, num_results)
                logger.info(f"âœ… åˆæˆçµæœã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(synthetic_results)} ä»¶")
                return synthetic_results
            
            if search_results:
                logger.info(f"âœ… æ¤œç´¢å®Œäº†: {len(search_results)} ä»¶ã®çµæœã‚’å–å¾—")
            else:
                logger.warning(f"âš ï¸ æ¤œç´¢çµæœãªã—: æ¤œç´¢ã¯æ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # Check if we have fewer results than requested
            if len(search_results) < num_results:
                logger.info(f"âš ï¸ è¦æ±‚ã•ã‚ŒãŸçµæœæ•° ({num_results}) ã‚ˆã‚Šå°‘ãªã„çµæœ ({len(search_results)}) ã—ã‹å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                # Add synthetic results to reach the requested number
                additional_needed = num_results - len(search_results)
                if additional_needed > 0:
                    logger.info(f"è¿½åŠ çµæœã‚’ç”Ÿæˆã—ã¾ã™: {additional_needed} ä»¶")
                    synthetic_results = generate_synthetic_results(query, additional_needed)
                    search_results.extend(synthetic_results)
                    logger.info(f"âœ… åˆè¨ˆ: {len(search_results)} ä»¶ã®çµæœ")
            
            # Limit results to requested number
            return search_results[:num_results]
            
        except Exception as e:
            # Check if it's a quota error
            error_message = str(e)
            if ("insufficient_quota" in error_message or 
                "billing" in error_message.lower() or 
                "credit balance" in error_message.lower() or 
                "credit_balance" in error_message.lower()):
                logger.error(f"âŒ APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                # Update the API quota status for future calls
                self.api_quota_available = False
                logger.info("âŒ APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¸è¶³ã®ãŸã‚ã€å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
                raise Exception("APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¸è¶³ã®ãŸã‚æ¤œç´¢ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            else:
                # For other errors, simply report the error and stop
                logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                logger.info("âŒ æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
                raise Exception(f"æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_message[:200]}")
    
    def search_deep(self, topic: str, depth: str = "medium", primary_results_count: int = 30) -> ResearchResult:
        """
        Perform a multi-layer search on a topic.
        
        Args:
            topic: The main topic to research
            depth: Search depth - "low", "medium", or "high"
            primary_results_count: Number of initial search results to retrieve
            
        Returns:
            ResearchResult containing all search findings and summary
        """
        logger.info(f"ğŸš€ ãƒˆãƒ”ãƒƒã‚¯ã€Œ{topic}ã€ã®èª¿æŸ»ã‚’é–‹å§‹ (æ·±ã•: {depth})")
        
        # Initialize research result
        research_result = ResearchResult(topic=topic)
        
        # Set secondary search depth based on primary depth
        secondary_search_depth = depth
        
        # Set the count of secondary searches based on depth
        # For 'low' depth, we'll search until we find 3 valid results instead of a fixed number
        required_sources = {
            "low": 3,     # Find at least 3 sources for low depth
            "medium": 3,  # Default number
            "high": 5     # More secondary searches
        }.get(depth, 3)
        
        secondary_search_count = required_sources  # Default behavior for medium/high
        
        # Set number of layers based on depth
        layers = {
            "low": 1,     # Only primary search
            "medium": 2,  # Primary + some secondary
            "high": 2     # Primary + extensive secondary
        }.get(depth, 2)
        
        # Try alternative search queries if the topic contains non-English characters
        search_queries = [topic]
        
        # Add translated queries if needed
        if any(ord(c) > 127 for c in topic):
            try:
                # Generate alternative search queries using AI
                response = client.chat.completions.create(
                    model=self._get_appropriate_model(depth),
                    messages=[
                        {"role": "system", "content": "You are a multilingual translation assistant. Convert the given query into English if it's not already in English, preserving the original meaning."},
                        {"role": "user", "content": f"Translate if needed: {topic}"}
                    ],
                    max_tokens=100
                )
                
                translated = response.choices[0].message.content.strip()
                if translated.lower() != topic.lower():
                    search_queries.append(translated)
                    logger.info(f"ğŸ”¤ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : {translated}")
                    
                    # Also add more specific variants
                    search_queries.append(f"{translated} guide")
                    search_queries.append(f"{translated} tutorial")
                
            except Exception as e:
                # ç¿»è¨³ã«å¤±æ•—ã—ã¦ã‚‚ã€å‡¦ç†ã‚’åœæ­¢ã›ãšã«ä»£æ›¿ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
                logger.warning(f"âš ï¸ æ¤œç´¢ã‚¯ã‚¨ãƒªå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚ˆãã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã®å ´åˆã¯ã€æ‰‹å‹•ã§ç¿»è¨³ã‚’è©¦ã¿ã‚‹
                if "é‡å­" in topic:
                    search_queries.append("Quantum computing")
                    search_queries.append("Quantum physics")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Quantum computing")
                elif "äººå·¥çŸ¥èƒ½" in topic or "AI" in topic.upper():
                    search_queries.append("Artificial Intelligence")
                    search_queries.append("AI technology")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Artificial Intelligence")
                elif "æ©Ÿæ¢°å­¦ç¿’" in topic:
                    search_queries.append("Machine Learning")
                    search_queries.append("ML algorithms")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Machine Learning")
                # ãã®ä»–ã®ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯
                else:
                    # è‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                    search_queries.append("guide tutorial")
                    search_queries.append("introduction overview")
                    logger.info("ğŸ”¤ ä¸€èˆ¬çš„ãªè‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        
        # First layer: Primary search - try multiple queries until we get results
        all_primary_results = []
        
        for query in search_queries:
            logger.info(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ: {query}")
            primary_results = self.search_web(query, num_results=primary_results_count, depth=depth)
            
            if primary_results:
                all_primary_results.extend(primary_results)
                
                # For "low" depth, check if we have enough unique sources
                if depth == "low":
                    # Count unique domains in our results
                    unique_domains = set()
                    for result in all_primary_results:
                        try:
                            from urllib.parse import urlparse
                            domain = urlparse(result.url).netloc
                            unique_domains.add(domain)
                        except:
                            # If URL parsing fails, count the whole URL
                            unique_domains.add(result.url)
                    
                    if len(unique_domains) >= required_sources:
                        logger.info(f"âœ… '{depth}' æ·±åº¦ã§å¿…è¦ãª {required_sources} ä»¶ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºä¿")
                        break
                # For other depths, use the original logic
                elif len(all_primary_results) >= primary_results_count // 2:
                    logger.info("âœ… ååˆ†ãªæ¤œç´¢çµæœã‚’å–å¾—")
                    break  # We have enough results, stop trying more queries
        
        # Deduplicate results
        seen_urls = set()
        unique_primary_results = []
        for result in all_primary_results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_primary_results.append(result)
        
        # Evaluate credibility of primary results
        primary_results = evaluate_credibility(unique_primary_results, self._get_appropriate_model("low"))
        
        # Cluster to reduce redundancy
        primary_results = cluster_results(primary_results)
        research_result.primary_results = primary_results
        
        if layers >= 2 and primary_results:
            # Extract knowledge gaps for secondary searches
            logger.info("ğŸ”¬ è¿½åŠ èª¿æŸ»é …ç›®ã‚’ç‰¹å®šä¸­...")
            knowledge_gaps = extract_knowledge_gaps(
                primary_results, 
                topic, 
                depth, 
                self._get_appropriate_model(depth)
            )
            research_result.knowledge_gaps = knowledge_gaps
            
            # Second layer: Search for each knowledge gap
            logger.info("ğŸ” è©³ç´°æƒ…å ±ã®æ¤œç´¢ã‚’é–‹å§‹...")
            secondary_results = []
            
            # Use only a subset of knowledge gaps based on depth
            for gap in knowledge_gaps[:secondary_search_count]:
                # Create a more specific search query
                for base_query in search_queries[:1]:  # Use only the first query as base to limit requests
                    specific_query = f"{base_query} {gap}"
                    logger.info(f"ğŸ” è©³ç´°æ¤œç´¢: {specific_query}")
                gap_results = self.search_web(specific_query, num_results=5, depth=secondary_search_depth)
                
                # Evaluate and add to secondary results
                if gap_results:
                    gap_results = evaluate_credibility(gap_results, self._get_appropriate_model("low"))
                    secondary_results.extend(gap_results)
                    logger.info(f"âœ… è©³ç´°æƒ…å ±ã‚’å–å¾—: {len(gap_results)} ä»¶")
                    break  # If we got results, don't try other base queries
            
            # Cluster secondary results
            if secondary_results:
                secondary_results = cluster_results(secondary_results)
                research_result.secondary_results = secondary_results
        
        # If we still have no results (primary or secondary), generate basic knowledge
        if not research_result.primary_results and not research_result.secondary_results:
            logger.warning("âš ï¸ æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬æƒ…å ±ã‚’ç”Ÿæˆã—ã¾ã™")
            synthetic_results = generate_synthetic_results(topic, 5)
            research_result.primary_results = synthetic_results
            
            # Also generate some knowledge gaps
            research_result.knowledge_gaps = [
                f"Fundamentals of {topic}",
                f"Applications of {topic}",
                f"Best practices for {topic}",
                f"Future developments in {topic}"
            ]
        
        # Ensure knowledge_gaps doesn't exceed reasonable size to prevent truncation
        if research_result.knowledge_gaps and len(research_result.knowledge_gaps) > 0:
            # Check each knowledge gap for length and truncate if too long
            for i in range(len(research_result.knowledge_gaps)):
                if len(research_result.knowledge_gaps[i]) > 500:
                    logger.warning(f"âš ï¸ çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—é …ç›®ãŒé•·ã™ãã¾ã™ã€‚åˆ‡ã‚Šè©°ã‚ã¾ã™: {research_result.knowledge_gaps[i][:50]}...")
                    research_result.knowledge_gaps[i] = research_result.knowledge_gaps[i][:500] + "..."
            
            # Limit total number of knowledge gaps
            if len(research_result.knowledge_gaps) > 10:
                logger.warning(f"âš ï¸ çŸ¥è­˜ã‚®ãƒ£ãƒƒãƒ—ãŒå¤šã™ãã¾ã™ã€‚ä¸Šä½10ä»¶ã«é™å®šã—ã¾ã™")
                research_result.knowledge_gaps = research_result.knowledge_gaps[:10]
        
        # Generate a comprehensive summary
        logger.info("ğŸ“ èª¿æŸ»çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆä¸­...")
        research_result.summary = generate_summary(research_result, self.model)
        
        logger.info("âœ… ãƒˆãƒ”ãƒƒã‚¯èª¿æŸ»å®Œäº†")
        return research_result 