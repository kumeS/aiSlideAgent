"""
Research Agent

Implements multi-layer web search and content analysis for comprehensive topic research.
"""

import os
import json
import requests
from typing import Dict, List, Any, Optional, Tuple
import hashlib
from urllib.parse import urlparse
from pydantic import BaseModel, Field
import logging
from dotenv import load_dotenv
import time
import random
import re

from agents import client, DEFAULT_MODEL

# Configure logging - ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã¯ agents/__init__.py ã«é›†ç´„
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
SERP_API_KEY = os.getenv("SERP_API_KEY")

class SearchResult(BaseModel):
    """Model representing a search result with metadata."""
    url: str
    title: str
    snippet: str
    content: Optional[str] = None
    source_type: str = Field(default="web")
    credibility_score: float = Field(default=0.0)
    domain: str = ""
    url_hash: str = ""
    
    def __init__(self, **data):
        super().__init__(**data)
        # Extract domain from URL
        parsed_url = urlparse(self.url)
        self.domain = parsed_url.netloc
        # Create URL hash for clustering
        self.url_hash = hashlib.md5(self.url.encode()).hexdigest()

class ResearchResult(BaseModel):
    """Model representing the complete research results.

    Flow2 ã§ææ¡ˆã•ã‚ŒãŸ DataModel v2 ã«åŸºã¥ãã€æ¤œç´¢çµæœã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¦å¾Œç¶šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ç›´æ¥å¼•ç”¨ã§ãã‚‹ã‚ˆã†ã€
    `raw_chunks` ã¨ `embeddings` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã€‚"""

    topic: str
    primary_results: List[SearchResult] = Field(default_factory=list)
    secondary_results: List[SearchResult] = Field(default_factory=list)
    summary: str = ""
    knowledge_gaps: List[str] = Field(default_factory=list)
    # --- Flow2 additions ---
    raw_chunks: List[str] = Field(default_factory=list, description="ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å›é¿ã®ãŸã‚ã«åˆ†å‰²ã•ã‚ŒãŸæ¤œç´¢çµæœã®æŠœç²‹")
    embeddings: Optional[List[float]] = Field(default=None, description="æ¤œç´¢çµæœã‚µãƒãƒªãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")

class ResearchAgent:
    """Agent for performing multi-layer web research on topics."""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """Initialize the research agent with API keys and configuration."""
        self.serp_api_key = api_key or SERP_API_KEY
        self.model = model or DEFAULT_MODEL
        self.max_retries = 3
        self.fallback_mode = False
        
        # Use a less expensive model for search to avoid quota issues
        self.search_model = os.getenv("OPENAI_SEARCH_MODEL", "gpt-3.5-turbo-0125")
        
        # Alternative model for fallbacks
        self.fallback_model = os.getenv("OPENAI_FALLBACK_MODEL", "gpt-3.5-turbo-0125")
        
        # Check if models are available by testing API (without making a full search)
        self.api_quota_available = self._check_api_quota()
        
        # Comment out this forced fallback setting to allow API usage when available
        # self.fallback_mode = True
    
    def _check_api_quota(self) -> bool:
        """Check if we have available API quota by making a minimal API call."""
        try:
            # Try with the cheapest model first
            ultra_cheap_model = "gpt-3.5-turbo-0125"  # Always use the cheapest model for testing
            
            # Make a minimal API call with minimal tokens to check if we have quota
            response = client.chat.completions.create(
                model=ultra_cheap_model,
                messages=[
                    {"role": "system", "content": "Test."},
                    {"role": "user", "content": "Hi"}
                ],
                max_tokens=1
            )
            logger.info("âœ… APIæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ: OpenAI APIãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return True
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                logger.warning("âš ï¸ OpenAI APIã®ã‚¯ã‚©ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ä»£æ›¿ã®æ¤œç´¢æ‰‹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            elif "billing" in error_message.lower():
                logger.warning("âš ï¸ OpenAI APIã®æ”¯æ‰•ã„é–¢é€£ã®å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ä»£æ›¿ã®æ¤œç´¢æ‰‹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            elif "credit balance" in error_message.lower() or "credit_balance" in error_message.lower():
                logger.warning("âš ï¸ OpenAI APIã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãƒãƒ©ãƒ³ã‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ä»£æ›¿ã®æ¤œç´¢æ‰‹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            else:
                # Other errors might be temporary, so we'll try to use the API anyway
                logger.warning(f"âš ï¸ APIæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€æ¤œç´¢ã¯è©¦è¡Œã—ã¾ã™: {e}")
                return True
    
    def _get_appropriate_model(self, task_importance: str = "medium") -> str:
        """
        Get the appropriate model based on task importance and available budget.
        
        Args:
            task_importance: "high", "medium", or "low" indicating task criticality
            
        Returns:
            Model name to use for this task
        """
        # If API quota is not available, always use the fallback model
        if not self.api_quota_available:
            return self.fallback_model
        
        # Map task importance to models
        if task_importance == "high":
            # For critical tasks (outline generation, final summaries)
            return self.model
        elif task_importance == "medium":
            # For important but not critical tasks (search, credibility)
            return self.search_model
        else:
            # For less critical tasks (gap analysis, clustering)
            return self.fallback_model
    
    def search_web(self, query: str, num_results: int = 10, depth: str = "medium") -> List[SearchResult]:
        """
        Perform a web search and return structured results.
        Primarily uses OpenAI's search-enabled models.
        
        Args:
            query: The search query
            num_results: Number of results to return
            depth: Search depth - "low", "medium", or "high"
        """
        # Check if we have API quota
        if not self.api_quota_available:
            logger.warning("âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€ä»£æ›¿æ¤œç´¢æ–¹æ³•ã‚’è©¦è¡Œã—ã¾ã™")
            
            # First try using an alternative search provider if available
            alternative_results = self._search_with_alternative_provider(query, num_results)
            if alternative_results:
                logger.info(f"âœ… ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰ {len(alternative_results)} ä»¶ã®çµæœã‚’å–å¾—")
                return alternative_results
            
            # If alternative search also fails, fall back to offline generation
            logger.warning("âš ï¸ ä»£æ›¿æ¤œç´¢ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç”Ÿæˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
            return self._generate_offline_results(query, num_results)
            
        # If we have quota, try OpenAI search
        return self._search_with_openai(query, num_results, depth)
    
    def _generate_offline_results(self, query: str, num_results: int = 5) -> List[SearchResult]:
        """Generate comprehensive results without using OpenAI API when quota is exceeded."""
        logger.info(f"âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ãªã—ã§ã®å‡¦ç†ãŒã§ãã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
        raise Exception("APIã‚¯ã‚©ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
    
    def _search_with_openai(self, query: str, num_results: int = 10, depth: str = "medium") -> List[SearchResult]:
        """Use OpenAI's search-enabled model to perform web searches."""
        logger.info(f"ğŸ” æ¤œç´¢ä¸­: {query} (æ·±ã•: {depth})")
        
        try:
            # å‹å®‰å…¨æ€§ã®ç¢ºä¿: depth ãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
            if not isinstance(depth, str):
                logger.warning(f"âš ï¸ æ¤œç´¢æ·±åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¾ã™: {depth} â†’ 'medium'")
                depth = "medium"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«è¨­å®š
                
            # Map depth string to search_context_size
            search_depth = {
                "low": "low",
                "medium": "medium",
                "high": "high",
                # Add support for alternative values that might be passed
                "small": "low",
                "large": "high",
                "shallow": "low",
                "deep": "high"
            }.get(depth.lower(), "medium")
            
            # Log the actual search depth being used
            if depth.lower() != search_depth:
                logger.info(f"ğŸ” æ¤œç´¢æ·±åº¦ã®ãƒãƒƒãƒ”ãƒ³ã‚°: '{depth}' â†’ '{search_depth}'")
            
            # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
            # æ¤œç´¢å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã ã‘ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            search_capable_models = ["gpt-4o-search-preview", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-2024-04-09"]
            
            # å¯èƒ½ãªé™ã‚Šã€æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            search_model = None
            
            # ã¾ãšç’°å¢ƒå¤‰æ•°ã§æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
            for model_name in search_capable_models:
                if os.getenv(f"OPENAI_USE_{model_name.replace('-', '_').upper()}", "false").lower() == "true":
                    search_model = model_name
                    logger.info(f"ğŸ” ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€Œ{search_model}ã€ã‚’ä½¿ç”¨")
                    break
            
            # ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šãŒãªã‘ã‚Œã°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            if not search_model:
                # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
                search_model = "gpt-4o-search-preview"  # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥æŒ‡å®š
                logger.info(f"ğŸ” æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€Œ{search_model}ã€ã‚’ä½¿ç”¨")
            
            # æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ä¸å¯ã¨æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä»£æ›¿æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if os.getenv("OPENAI_DISABLE_SEARCH", "false").lower() == "true":
                logger.warning(f"âš ï¸ æ¤œç´¢æ©Ÿèƒ½ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»£æ›¿æ–¹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return self._generate_synthetic_results(query, num_results)
            
            # é‡è¤‡ã—ãŸãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦å‰Šé™¤
            # logger.info(f"ğŸ” æ¤œç´¢å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€Œ{search_model}ã€ã‚’ä½¿ç”¨")
            
            # Construct a prompt that will return structured search results
            system_prompt = """
            You are a search engine assistant that helps find and summarize information from the web.
            Search the web for the query, and return results in this EXACT format for each result:

            ---
            Title: [ACTUAL PAGE TITLE, not just domain name]
            URL: [FULL URL]
            Summary: [2-3 sentence summary of the actual page content, not just meta description]
            ---

            Provide real page titles - not just domain names. 
            Summaries should be informative and based on actual content.
            Do not use markdown formatting in your response.
            """
            
            # Retry with slight variations if needed
            max_search_retries = 2
            search_retry_count = 0
            response = None
            
            while search_retry_count <= max_search_retries and response is None:
                try:
                    # Make the API request with web search enabled
                    query_to_use = query
                    
                    # On retry, add more context to help the search
                    if search_retry_count == 1:
                        query_to_use = f"detailed information about {query}"
                    elif search_retry_count == 2:
                        query_to_use = f"{query} guide tutorial information"
                    
                    response = client.chat.completions.create(
                        model=search_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": f"Search for: {query_to_use}"}
                        ],
                        max_tokens=1500,
                        web_search_options={
                            "search_context_size": search_depth,  # Use the mapped depth
                            "user_location": {
                                "type": "approximate",
                                "approximate": {
                                    "country": "JP",  # Default to Japan
                                },
                            },
                        }
                    )
                except Exception as search_error:
                    search_retry_count += 1
                    # Format error message to be more user-friendly
                    error_str = str(search_error)
                    
                    # Extract and simplify common error messages
                    if "invalid_value" in error_str and "web_search_options.search_context_size" in error_str:
                        simplified_error = "æ¤œç´¢æ·±åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™ã€‚æœ‰åŠ¹ãªå€¤: 'low', 'medium', 'high'"
                    elif "not supported with this model" in error_str and "Web search" in error_str:
                        simplified_error = f"ãƒ¢ãƒ‡ãƒ« '{search_model}' ã¯ã‚¦ã‚§ãƒ–æ¤œç´¢ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™ã€‚"
                    elif "insufficient_quota" in error_str or "billing" in error_str.lower():
                        simplified_error = "APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä»£æ›¿æ¤œç´¢ã‚’è©¦è¡Œã—ã¾ã™ã€‚"
                    elif "Rate limit" in error_str:
                        simplified_error = "ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆåˆ¶é™: çŸ­æ™‚é–“ã«å¤šãã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒè¡Œã‚ã‚Œã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¾ã™ã€‚"
                    elif "auth" in error_str.lower() and "error" in error_str.lower():
                        simplified_error = "èªè¨¼ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    else:
                        # Keep original error but limit verbosity
                        shortened_error = error_str[:100] + "..." if len(error_str) > 100 else error_str
                        simplified_error = f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {shortened_error}"
                    
                    logger.warning(f"æ¤œç´¢è©¦è¡Œ {search_retry_count}: {simplified_error}")
                    
                    if search_retry_count > max_search_retries:
                        # ã™ã¹ã¦ã®æ¤œç´¢ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                        logger.error(f"æ¤œç´¢å¤±æ•—: 3å›è©¦è¡Œã—ã¾ã—ãŸãŒæˆåŠŸã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_str}")
                        logger.info("ä»£æ›¿æƒ…å ±ç”Ÿæˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
                        raise  # Re-raise if we've exhausted our retries
                    time.sleep(2)  # Brief pause before retrying
            
            if response is None:
                raise Exception("All search attempts failed")
            
            # Extract search results from the response
            content = response.choices[0].message.content.strip()
            
            # Log the raw response for debugging
            logger.debug(f"Raw search response: {content[:1000]}...")
            
            # Parse the results using the clean format we specified
            search_results = []
            result_blocks = content.split("---")
            
            for block in result_blocks:
                if not block.strip():
                    continue
                    
                title_match = re.search(r'Title:\s*(.*?)(?:\n|$)', block, re.IGNORECASE)
                url_match = re.search(r'URL:\s*(https?://\S+)(?:\n|$)', block, re.IGNORECASE)
                summary_match = re.search(r'Summary:\s*([\s\S]*?)(?=\n\s*$|\Z)', block, re.IGNORECASE)
                
                if title_match and url_match:
                    title = title_match.group(1).strip()
                    url = url_match.group(1).strip()
                    summary = summary_match.group(1).strip() if summary_match else "No summary available"
                    
                    search_result = SearchResult(
                        url=url,
                        title=title,
                        snippet=summary,
                        source_type="ai_search"
                    )
                    search_results.append(search_result)
            
            # If no results from blocks, try older patterns
            if not search_results:
                # Try to match Title, URL, Summary pattern
                title_url_summary_pattern = r'(?:^|\n)(?:\d+\.\s*)?Title:\s*(.*?)(?:\n|\r\n)URL:\s*(https?://\S+)(?:\n|\r\n)Summary:\s*((?:.|\n)*?)(?=(?:^|\n)(?:\d+\.\s*)?Title:|$)'
                matches = re.findall(title_url_summary_pattern, content, re.MULTILINE | re.DOTALL)
                
                if matches:
                    for match in matches:
                        if len(match) >= 3:
                            title = match[0].strip()
                            url = match[1].strip()
                            summary = match[2].strip()
                            
                            search_result = SearchResult(
                                url=url,
                                title=title,
                                snippet=summary,
                                source_type="ai_search"
                            )
                            search_results.append(search_result)
            
            # If still no results, try JSON pattern
            if not search_results:
                json_pattern = r'\{[^{}]*"title"[^{}]*"url"[^{}]*"snippet"[^{}]*\}'
                json_objects = re.findall(json_pattern, content, re.DOTALL)
                
                if json_objects:
                    for json_str in json_objects:
                        try:
                            cleaned_json = re.sub(r'([{,]\s*)(\w+)(\s*:)', r'\1"\2"\3', json_str)
                            cleaned_json = cleaned_json.replace("'", '"')
                            result_data = json.loads(cleaned_json)
                            
                            if "title" in result_data and "url" in result_data and "snippet" in result_data:
                                search_result = SearchResult(
                                    url=result_data["url"],
                                    title=result_data["title"],
                                    snippet=result_data["snippet"],
                                    source_type="ai_search"
                                )
                                search_results.append(search_result)
                        except Exception as e:
                            logger.warning(f"Failed to parse JSON object: {e}")
            
            # If still no results, try numbered list pattern
            if not search_results:
                result_pattern = r'(?:\d+[\.\)]\s*|â€¢\s*|\*\s*)(.*?)\n\s*(?:URL|Link):\s*(https?://\S+)\s*(?:\n|$).*?(?:Snippet|Summary|Description):\s*(.*?)(?:\n\s*(?:\d+[\.\)]|â€¢|\*)|$)'
                matches = re.findall(result_pattern, content, re.DOTALL)
                
                for match in matches:
                    if len(match) >= 3:
                        title = match[0].strip()
                        url = match[1].strip()
                        snippet = match[2].strip()
                        
                        search_result = SearchResult(
                            url=url,
                            title=title,
                            snippet=snippet,
                            source_type="ai_search"
                        )
                        search_results.append(search_result)
            
            # If still no results, extract URLs
            if not search_results:
                url_pattern = r'((?:https?://)[^\s\)]+)'
                urls = re.findall(url_pattern, content)
                
                for url in urls:
                    # Find context around this URL
                    url_pos = content.find(url)
                    context_start = max(0, content.rfind('\n', 0, url_pos))
                    context_end = content.find('\n', url_pos + len(url))
                    if context_end == -1:
                        context_end = len(content)
                    
                    context = content[context_start:context_end].strip()
                    
                    # Extract title - assume it's at the beginning of the context
                    title_end = min(100, len(context))
                    title = context[:title_end].strip()
                    if len(title) > 50:
                        title = title[:50] + "..."
                    
                    # Use remaining context as snippet
                    snippet = context[len(title):].strip()
                    if not snippet:
                        snippet = f"Information about {query}"
                    
                    search_result = SearchResult(
                        url=url,
                        title=title,
                        snippet=snippet,
                        source_type="ai_search"
                    )
                    search_results.append(search_result)
            
            # If we still have no results, generate synthetic results
            if not search_results:
                logger.warning(f"âš ï¸ ã‚¦ã‚§ãƒ–æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆæˆçµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                synthetic_results = self._generate_synthetic_results(query, num_results)
                logger.info(f"âœ… åˆæˆçµæœã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(synthetic_results)} ä»¶")
                return synthetic_results
            
            if search_results:
                logger.info(f"âœ… æ¤œç´¢å®Œäº†: {len(search_results)} ä»¶ã®çµæœã‚’å–å¾—")
            else:
                logger.warning(f"âš ï¸ æ¤œç´¢çµæœãªã—: æ¤œç´¢ã¯æ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # Limit results to requested number
            return search_results[:num_results]
            
        except Exception as e:
            # Check if it's a quota error
            error_message = str(e)
            if ("insufficient_quota" in error_message or 
                "billing" in error_message.lower() or 
                "credit balance" in error_message.lower() or 
                "credit_balance" in error_message.lower()):
                logger.error(f"âŒ APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                # Update the API quota status for future calls
                self.api_quota_available = False
                logger.info("âŒ APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¸è¶³ã®ãŸã‚ã€å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
                raise Exception("APIã‚¯ã‚©ãƒ¼ã‚¿/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¸è¶³ã®ãŸã‚æ¤œç´¢ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
            else:
                # For other errors, simply report the error and stop
                logger.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                logger.info("âŒ æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
                raise Exception(f"æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_message[:200]}")
    
    def _generate_synthetic_results(self, query: str, num_results: int = 5) -> List[SearchResult]:
        """Generate synthetic search results when real search fails.
        
        Args:
            query: The search query
            num_results: Number of synthetic results to generate
            
        Returns:
            List of synthetic SearchResult objects
        """
        logger.info(f"ğŸ¤– ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã«å¯¾ã™ã‚‹åˆæˆçµæœã‚’ç”Ÿæˆä¸­...")
        
        # è‹±èªã®ã‚¯ã‚¨ãƒªã‚’æƒ³å®šã—ãŸãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        topic = query.lower()
        if "quantum" in topic or "é‡å­" in topic:
            domain_content = {
                "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åŸºç¤": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ¬¡ä¸–ä»£ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã™ã€‚å¾“æ¥ã®ãƒ“ãƒƒãƒˆï¼ˆ0ã‹1ï¼‰ã§ã¯ãªãã€ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€é‡ã­åˆã‚ã›ã¨é‡å­ã‚‚ã¤ã‚Œã«ã‚ˆã‚Šä¸¦åˆ—è¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦ã‚·ãƒ§ã‚¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã‚°ãƒ­ãƒ¼ãƒãƒ¼ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ‰åã§ã™ã€‚",
                "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®å®Ÿç”¨åŒ–": "IBMã‚„Googleãªã©ã®ä¼æ¥­ãŒé‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®å®Ÿç”¨åŒ–ã«å‘ã‘ã¦ç ”ç©¶ã‚’é€²ã‚ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã¯æ•°åã€œæ•°ç™¾ã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã®å®Ÿé¨“çš„ãªãƒã‚·ãƒ³ãŒé–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ãŒã€ã‚¨ãƒ©ãƒ¼è¨‚æ­£ã‚„é‡å­ã‚²ãƒ¼ãƒˆã®ç²¾åº¦å‘ä¸ŠãŒèª²é¡Œã§ã™ã€‚",
                "é‡å­æš—å·": "é‡å­éµé…é€ï¼ˆQKDï¼‰ã¯ç›—è´ã‚’æ¤œçŸ¥ã§ãã‚‹å®‰å…¨ãªé€šä¿¡æ–¹å¼ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚é‡å­çŠ¶æ…‹ã®è¦³æ¸¬ã«ã‚ˆã‚ŠçŠ¶æ…‹ãŒå¤‰åŒ–ã™ã‚‹æ€§è³ªã‚’åˆ©ç”¨ã—ã€ç†è«–ä¸Šã¯å®Œå…¨ã«å®‰å…¨ãªæš—å·ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚",
                "é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": "é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚ˆã‚ŠåŠ¹ç‡çš„ã«è§£ã‘ã‚‹å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ç´ å› æ•°åˆ†è§£ï¼ˆã‚·ãƒ§ã‚¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã€æ¢ç´¢å•é¡Œï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ¼ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã€é‡å­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ãŒä»£è¡¨çš„ã§ã™ã€‚",
                "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®å¿œç”¨": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ææ–™ç§‘å­¦ã€è–¬ç‰©è¨­è¨ˆã€é‡‘èãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€æ©Ÿæ¢°å­¦ç¿’ã€æœ€é©åŒ–å•é¡Œãªã©ã®åˆ†é‡ã§é©æ–°çš„ãªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«è¤‡é›‘ãªé‡å­ç³»ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã¯å›°é›£ã§ã™ã€‚"
            }
        elif "ai" in topic or "machine learning" in topic or "äººå·¥çŸ¥èƒ½" in topic or "æ©Ÿæ¢°å­¦ç¿’" in topic:
            domain_content = {
                "äººå·¥çŸ¥èƒ½ã®åŸºç¤": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã—ã€å­¦ç¿’ã€æ¨è«–ã€è‡ªå·±ä¿®æ­£èƒ½åŠ›ã‚’æŒã¤ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯AIã®ä¸»è¦ãªæ‰‹æ³•ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã„ã¾ã™ã€‚",
                "æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": "æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ãŒæ©Ÿæ¢°å­¦ç¿’ã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™ã€‚å›å¸°åˆ†æã€æ±ºå®šæœ¨ã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ã®æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚",
                "æ·±å±¤å­¦ç¿’ã®ç™ºå±•": "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å±¤ã‚’æ·±ãã—ãŸæ‰‹æ³•ã§ã€ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã€éŸ³å£°èªè­˜ãªã©ã§é©å‘½çš„ãªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚CNNã‚„RNNã€Transformerãªã©ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒä»£è¡¨çš„ã§ã™ã€‚",
                "AIã®å€«ç†ã¨ç¤¾ä¼šçš„å½±éŸ¿": "AIã®ç™ºå±•ã«ä¼´ã„ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€ãƒã‚¤ã‚¢ã‚¹ã€è‡ªå‹•åŒ–ã«ã‚ˆã‚‹é›‡ç”¨å¤‰åŒ–ã€æ„æ€æ±ºå®šã®é€æ˜æ€§ãªã©ã®èª²é¡ŒãŒç”Ÿã˜ã¦ã„ã¾ã™ã€‚è²¬ä»»ã‚ã‚‹AIé–‹ç™ºã¨åˆ©ç”¨ã®ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚„è¦åˆ¶ãŒè­°è«–ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                "AIã®å¿œç”¨åˆ†é‡": "åŒ»ç™‚è¨ºæ–­ã€è‡ªå‹•é‹è»¢è»Šã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€è‡ªç„¶è¨€èªå‡¦ç†ã€ãƒ­ãƒœãƒƒãƒˆå·¥å­¦ãªã©ã€AIã¯å¤šæ§˜ãªåˆ†é‡ã§é©æ–°ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«GPTã®ã‚ˆã†ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯æ§˜ã€…ãªèª²é¡Œã«å¯¾å¿œã§ãã‚‹æ±ç”¨æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
            }
        else:
            # ãã®ä»–ã®ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã™ã‚‹ä¸€èˆ¬çš„ãªåˆæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            domain_content = {
                f"{query}ã®æ¦‚è¦": f"{query}ã¯ç¾ä»£ã®æŠ€è¡“ã‚„ç ”ç©¶ã«ãŠã„ã¦é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã§ã™ã€‚åŸºæœ¬çš„ãªæ¦‚å¿µã‹ã‚‰å¿œç”¨ã¾ã§å¹…åºƒã„çŸ¥è­˜ä½“ç³»ãŒã‚ã‚Šã¾ã™ã€‚",
                f"{query}ã®æ­´å²": f"{query}ã®åˆ†é‡ã¯é•·ã„æ­´å²ã‚’æŒã¡ã€æ™‚ä»£ã¨ã¨ã‚‚ã«ç™ºå±•ã—ã¦ãã¾ã—ãŸã€‚åˆæœŸã®æ¦‚å¿µã‹ã‚‰ç¾åœ¨ã®æœ€å…ˆç«¯ç ”ç©¶ã¾ã§æ§˜ã€…ãªé€²åŒ–ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚",
                f"{query}ã®å¿œç”¨": f"{query}ã¯ç§‘å­¦ã€æŠ€è¡“ã€ç¤¾ä¼šãªã©å¤šãã®åˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿç”¨çš„ãªä¾‹ã¨ã—ã¦ã¯æ§˜ã€…ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ãŒã‚ã‚Šã¾ã™ã€‚",
                f"{query}ã®å°†æ¥å±•æœ›": f"{query}ã®åˆ†é‡ã¯ä»Šå¾Œã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚æ–°ã—ã„æŠ€è¡“ã‚„æ–¹æ³•è«–ã«ã‚ˆã‚Šã€ç¾åœ¨ã®èª²é¡ŒãŒè§£æ±ºã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
                f"{query}ã«ãŠã‘ã‚‹èª²é¡Œ": f"{query}ã«é–¢ã™ã‚‹ç ”ç©¶ã‚„å¿œç”¨ã«ã¯ã€ã„ãã¤ã‹ã®é‡è¦ãªèª²é¡ŒãŒå­˜åœ¨ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®èª²é¡Œè§£æ±ºãŒä»Šå¾Œã®é€²å±•ã®éµã¨ãªã‚Šã¾ã™ã€‚"
            }
        
        # åˆæˆçµæœã®ç”Ÿæˆ
        synthetic_results = []
        domains = ["research.example.org", "academy.example.com", "science.example.net", 
                   "knowledge.example.edu", "institute.example.org"]
        
        items = list(domain_content.items())
        # è¦æ±‚ã•ã‚ŒãŸçµæœæ•°ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª¿æ•´ï¼ˆå°‘ãªãã¨ã‚‚1ã¤ã¯è¿”ã™ï¼‰
        for i in range(min(num_results, len(items))):
            topic_key, content = items[i]
            domain = domains[i % len(domains)]
            
            result = SearchResult(
                url=f"https://{domain}/{query.replace(' ', '-').lower()}/{i+1}",
                title=f"{topic_key} - æ•™è‚²ãƒªã‚½ãƒ¼ã‚¹",
                snippet=content[:150] + "...",
                content=content,
                source_type="synthetic",
                credibility_score=0.7
            )
            # domainã¨url_hashã‚’è¨­å®š
            result.domain = domain
            result.url_hash = hashlib.md5(result.url.encode()).hexdigest()
            
            synthetic_results.append(result)
        
        return synthetic_results
    
    def evaluate_credibility(self, results: List[SearchResult]) -> List[SearchResult]:
        """
        Evaluate the credibility of search results and assign credibility scores.
        Uses both URL-based heuristics and content-based LLM evaluation.
        """
        logger.info("ğŸ§ æƒ…å ±ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ä¸­...")
        
        try:
            # Simple baseline credibility evaluation based on source type
            for result in results:
                # Start with a baseline score based on source type
                if result.source_type == "web":
                    result.credibility_score = 0.5  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸­ç«‹çš„ãª0.5ã«å¤‰æ›´
                elif result.source_type == "ai_search":
                    result.credibility_score = 0.5
                elif result.source_type == "offline_synthetic":
                    result.credibility_score = 0.4
                elif result.source_type == "synthetic":
                    result.credibility_score = 0.3
                else:
                    result.credibility_score = 0.3
                
                # Extract domain from URL for further analysis
                try:
                    url_obj = urlparse(result.url)
                    result.domain = url_obj.netloc
                except:
                    result.domain = ""
                
                # Adjust scores based on domain characteristics
                if result.domain:
                    # Academic and government sources tend to be more reliable
                    if ".edu" in result.domain or ".gov" in result.domain:
                        result.credibility_score += 0.15
                    # .org can be somewhat more credible, but not always
                    elif ".org" in result.domain:
                        result.credibility_score += 0.1
                    # Known reference sites
                    elif any(ref in result.domain for ref in ["wikipedia.org", "britannica.com", "scholarpedia.org"]):
                        result.credibility_score += 0.1
                    # News sources
                    elif any(news in result.domain for news in ["bbc", "nytimes", "reuters", "apnews", "theguardian"]):
                        result.credibility_score += 0.1
                    
                    # Blogs and less formal sources may be less reliable
                    if "blog." in result.domain or "blogs." in result.domain:
                        result.credibility_score -= 0.05
                    
                    # Social media tends to be less reliable for factual information
                    if any(social in result.domain for social in ["facebook", "twitter", "reddit", "instagram", "tiktok"]):
                        result.credibility_score -= 0.1
                    
                    # Create a URL hash for tracking
                    result.url_hash = hashlib.md5(result.url.encode()).hexdigest()
                
            # ----- æ–°æ©Ÿèƒ½: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§è©•ä¾¡ -----
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒåˆ©ç”¨å¯èƒ½ãªçµæœï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆï¼‰ã«å¯¾ã—ã¦LLMè©•ä¾¡ã‚’å®Ÿè¡Œ
            logger.info("ğŸ” ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§åˆ†æã‚’å®Ÿè¡Œä¸­...")
            
            # ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã«çµæœã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆAPIã‚³ãƒ¼ãƒ«ã‚’æ¸›ã‚‰ã™ãŸã‚ï¼‰
            batch_size = 3  # ä¸€åº¦ã«è©•ä¾¡ã™ã‚‹çµæœã®æ•°
            batches = [results[i:i + batch_size] for i in range(0, len(results), batch_size)]
            
            # å…¨ãƒãƒƒãƒã®å‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
            failed_batch_count = 0
            
            for batch_idx, batch in enumerate(batches):
                # ãƒãƒƒãƒå†…ã®å„çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é›†ç´„
                batch_texts = []
                
                for result in batch:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ä½¿ç”¨
                    content_text = result.content if result.content else result.snippet
                    # æœ€ä½é™ã®é•·ã•ãŒã‚ã‚‹ã‹ç¢ºèª
                    if len(content_text) > 30:  # æœ€ä½30æ–‡å­—ä»¥ä¸Šã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                        batch_texts.append({
                            "url": result.url,
                            "title": result.title,
                            "content": content_text[:1000]  # é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®1000æ–‡å­—ã ã‘ä½¿ç”¨
                        })
                
                # è©•ä¾¡ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã®ã¿APIå‘¼ã³å‡ºã—
                if batch_texts:
                    try:
                        # æœ€åˆã®ãƒãƒƒãƒå‡¦ç†é–‹å§‹æ™‚ã«ãƒ­ã‚°ã‚’è¡¨ç¤º
                        if batch_idx == 0:
                            logger.info(f"ä¿¡é ¼æ€§è©•ä¾¡ã‚’é–‹å§‹: å…¨ {len(batches)} ãƒãƒƒãƒã‚’å‡¦ç†ä¸­...")
                            
                        # OpenAI APIã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡
                        response = client.chat.completions.create(
                            model=self._get_appropriate_model("low"),  # ä½ãƒªã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ååˆ†
                            messages=[
                                {"role": "system", "content": """
                                ã‚ãªãŸã¯æƒ…å ±ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã¤ã„ã¦ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ã—ã¦ãã ã•ã„:
                                
                                1. äº‹å®Ÿç¢ºèªå¯èƒ½æ€§: æƒ…å ±ãŒæ¤œè¨¼å¯èƒ½ãªäº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹
                                2. å®¢è¦³æ€§: åè¦‹ã‚„ä¸»è¦³çš„æ„è¦‹ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹
                                3. å°‚é–€æ€§: å°‚é–€çš„ãªçŸ¥è­˜ã‚„æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
                                4. æœ€æ–°æ€§: æƒ…å ±ãŒæœ€æ–°ã‹ã©ã†ã‹ (æ—¥ä»˜ã‚„æ™‚é–“çš„æ–‡è„ˆã‹ã‚‰åˆ¤æ–­)
                                5. ä¸€è²«æ€§: å†…å®¹ã«çŸ›ç›¾ãŒãªã„ã‹
                                
                                å„URLã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã—ã¦ã€0.0ã€œ1.0ã®ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
                                0.0ã¯ã€Œã¾ã£ãŸãä¿¡é ¼ã§ããªã„ã€ã€1.0ã¯ã€Œéå¸¸ã«ä¿¡é ¼ã§ãã‚‹ã€ã‚’æ„å‘³ã—ã¾ã™ã€‚
                                
                                å¿…ãšæ¬¡ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
                                {
                                  "results": [
                                    {"url": "URL1", "credibility_score": 0.X, "reason": "ç°¡æ½”ãªç†ç”±"},
                                    {"url": "URL2", "credibility_score": 0.Y, "reason": "ç°¡æ½”ãªç†ç”±"}
                                  ]
                                }
                                """},
                                {"role": "user", "content": f"ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¿¡é ¼æ€§ã‚’JSONå½¢å¼ã§è©•ä¾¡ã—ã¦ãã ã•ã„: {json.dumps(batch_texts, ensure_ascii=False)}"}
                            ],
                            response_format={"type": "json_object"},
                            temperature=0.2,
                            max_tokens=800
                        )
                        
                        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
                        try:
                            content = response.choices[0].message.content
                            assessment = json.loads(content)
                            
                            # å„çµæœã®ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
                            for item in assessment.get("results", []):
                                url = item.get("url")
                                new_score = item.get("credibility_score")
                                reason = item.get("reason", "")
                                
                                # å¯¾å¿œã™ã‚‹çµæœã‚’è¦‹ã¤ã‘ã¦æ›´æ–°
                                for result in batch:
                                    if result.url == url and new_score is not None:
                                        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã‚‹
                                        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã‚’40%ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã‚’60%ã®é‡ã¿ã§çµ±åˆ
                                        domain_weight = 0.4
                                        content_weight = 0.6
                                        current_score = result.credibility_score
                                        
                                        # é‡ã¿ä»˜ãå¹³å‡ã§æ–°ã—ã„ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                        result.credibility_score = (current_score * domain_weight) + (new_score * content_weight)
                                        logger.debug(f"URL: {url} ã®ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°: {current_score:.2f} â†’ {result.credibility_score:.2f}, ç†ç”±: {reason}")
                            
                            # ãƒãƒƒãƒãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚ŒãŸ
                            failed_batch_count = 0  # æˆåŠŸã—ãŸã‚‰ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
                        
                        except json.JSONDecodeError:
                            failed_batch_count += 1
                            logger.debug("ä¿¡é ¼æ€§è©•ä¾¡ã®å¿œç­”ã‚’JSONã¨ã—ã¦è§£æã§ãã¾ã›ã‚“ã§ã—ãŸ")
                            if failed_batch_count == 1:  # æœ€åˆã®å¤±æ•—æ™‚ã®ã¿è­¦å‘Šã‚’è¡¨ç¤º
                                logger.warning("âš ï¸ ä¸€éƒ¨ã®ä¿¡é ¼æ€§è©•ä¾¡ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
                        except Exception as parse_err:
                            failed_batch_count += 1
                            logger.debug(f"ä¿¡é ¼æ€§è©•ä¾¡ã®å¿œç­”è§£æã‚¨ãƒ©ãƒ¼: {str(parse_err)}")
                            if failed_batch_count == 1:  # æœ€åˆã®å¤±æ•—æ™‚ã®ã¿è­¦å‘Šã‚’è¡¨ç¤º
                                logger.warning("âš ï¸ ä¸€éƒ¨ã®ä¿¡é ¼æ€§è©•ä¾¡ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
                    
                    except Exception as api_err:
                        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã¯æœ€å°é™ã®æƒ…å ±ã‚’è¡¨ç¤º
                        logger.debug(f"ä¿¡é ¼æ€§è©•ä¾¡ã®APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(api_err)}")
                        failed_batch_count += 1
                        
                        # æœ€åˆã®å¤±æ•—æ™‚ã®ã¿è­¦å‘Šã‚’è¡¨ç¤º
                        if failed_batch_count == 1:
                            logger.warning("âš ï¸ ä¸€éƒ¨ã®ä¿¡é ¼æ€§è©•ä¾¡ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
                        
                        # å…¨ã¦ã®ãƒãƒƒãƒãŒå¤±æ•—ã—ãŸå ´åˆ
                        if failed_batch_count >= len(batches):
                            logger.warning("âš ï¸ ä¿¡é ¼æ€§è©•ä¾¡ãŒå®Œå…¨ã«å¤±æ•—ã—ã¾ã—ãŸã€‚åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¾ã™")
                            # åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’é©ç”¨ï¼ˆæœ€ä½0.2ã€æœ€å¤§0.8ï¼‰
                            for r in results:
                                if r.credibility_score < 0.2:
                                    r.credibility_score = 0.3 + (hash(r.url) % 10) / 20  # URLãƒãƒƒã‚·ãƒ¥ã«åŸºã¥ãã‚ãšã‹ãªãƒ©ãƒ³ãƒ€ãƒ æ€§
            
            # æœ€çµ‚å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã™ã¹ã¦ã®ãƒãƒƒãƒå‡¦ç†å¾Œï¼‰
            if len(batches) > 1:
                logger.info("âœ… ä¿¡é ¼æ€§è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # æœ€çµ‚çš„ãªã‚¹ã‚³ã‚¢ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¦æœ‰åŠ¹ãªç¯„å›²ï¼ˆ0.1ã€œ0.9ï¼‰ã«åã‚ã‚‹
            for result in results:
                result.credibility_score = max(0.1, min(0.9, result.credibility_score))
            
            logger.info(f"âœ… ä¿¡é ¼æ€§è©•ä¾¡å®Œäº†: URL + ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¿¡é ¼æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("âŒ ä¿¡é ¼æ€§è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™")
            
            # Just return results with default scores if evaluation fails
            for result in results:
                if not hasattr(result, 'credibility_score') or result.credibility_score == 0:
                    result.credibility_score = 0.5
            
            return results
    
    def cluster_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Cluster results by domain to reduce redundancy."""
        logger.info("ğŸ“Š æ¤œç´¢çµæœã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ä¸­...")
        domain_map = {}
        clustered_results = []
        
        for result in results:
            if result.domain not in domain_map:
                domain_map[result.domain] = []
            domain_map[result.domain].append(result)
        
        # Take the highest credibility result from each domain
        for domain, domain_results in domain_map.items():
            best_result = max(domain_results, key=lambda x: x.credibility_score)
            clustered_results.append(best_result)
        
        # Sort by credibility score
        clustered_results.sort(key=lambda x: x.credibility_score, reverse=True)
        logger.info(f"âœ… ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å®Œäº†: {len(clustered_results)} ä»¶ã®çµæœ")
        return clustered_results
    
    def extract_knowledge_gaps(self, results: List[SearchResult], topic: str, depth: str = "medium") -> List[str]:
        """Identify knowledge gaps from the initial search results."""
        if not results:
            return [f"Basic information about {topic}"]
        
        logger.info("ğŸ” æƒ…å ±ã‚®ãƒ£ãƒƒãƒ—ã‚’åˆ†æä¸­...")
        
        # Combine content for analysis
        combined_text = f"Topic: {topic}\n\n"
        for result in results:
            combined_text += f"Title: {result.title}\nSnippet: {result.snippet}\n\n"
        
        try:
            # Call OpenAI API to identify knowledge gaps
            response = client.chat.completions.create(
                model=self._get_appropriate_model(depth),
                messages=[
                    {"role": "system", "content": "Identify 3-5 specific knowledge gaps or areas that need deeper research based on the provided information. Focus on technical or detailed aspects that are not well covered."},
                    {"role": "user", "content": combined_text}
                ],
                max_tokens=200
            )
            
            # Process the response to extract knowledge gaps
            gaps_text = response.choices[0].message.content.strip()
            
            # Split into individual gaps (assuming each gap is on a new line or numbered)
            gaps = [gap.strip() for gap in gaps_text.split("\n") if gap.strip()]
            
            # Clean up numbering if present
            cleaned_gaps = []
            for gap in gaps:
                # Remove numbering like "1.", "2.", etc.
                if gap[0].isdigit() and gap[1:3] in [". ", ") "]:
                    gap = gap[3:].strip()
                cleaned_gaps.append(gap)
            
            logger.info(f"âœ… æƒ…å ±ã‚®ãƒ£ãƒƒãƒ—åˆ†æå®Œäº†: {len(cleaned_gaps)} ä»¶ã®è¿½åŠ èª¿æŸ»é …ç›®ã‚’ç‰¹å®š")
            return cleaned_gaps
        
        except Exception as e:
            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
            logger.error(f"âŒ æƒ…å ±ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã¯ç°¡æ½”ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
            logger.info("âŒ æƒ…å ±ã‚®ãƒ£ãƒƒãƒ—ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚åŸºæœ¬çš„ãªé …ç›®ã‚’ä½¿ç”¨ã—ã¾ã™")
            return [f"More detailed information about {topic}"]
    
    def search_deep(self, topic: str, depth: str = "medium", primary_results_count: int = 30) -> ResearchResult:
        """
        Perform a multi-layer search on a topic.
        
        Args:
            topic: The main topic to research
            depth: Search depth - "low", "medium", or "high"
            primary_results_count: Number of initial search results to retrieve
            
        Returns:
            ResearchResult containing all search findings and summary
        """
        logger.info(f"ğŸš€ ãƒˆãƒ”ãƒƒã‚¯ã€Œ{topic}ã€ã®èª¿æŸ»ã‚’é–‹å§‹ (æ·±ã•: {depth})")
        
        # Initialize research result
        research_result = ResearchResult(topic=topic)
        
        # Set secondary search depth based on primary depth
        secondary_search_depth = depth
        
        # Set the count of secondary searches based on depth
        # For 'low' depth, we'll search until we find 3 valid results instead of a fixed number
        required_sources = {
            "low": 3,     # Find at least 3 sources for low depth
            "medium": 3,  # Default number
            "high": 5     # More secondary searches
        }.get(depth, 3)
        
        secondary_search_count = required_sources  # Default behavior for medium/high
        
        # Set number of layers based on depth
        layers = {
            "low": 1,     # Only primary search
            "medium": 2,  # Primary + some secondary
            "high": 2     # Primary + extensive secondary
        }.get(depth, 2)
        
        # Try alternative search queries if the topic contains non-English characters
        search_queries = [topic]
        
        # Add translated queries if needed
        if any(ord(c) > 127 for c in topic):
            try:
                # Generate alternative search queries using AI
                response = client.chat.completions.create(
                    model=self._get_appropriate_model(depth),
                    messages=[
                        {"role": "system", "content": "You are a multilingual translation assistant. Convert the given query into English if it's not already in English, preserving the original meaning."},
                        {"role": "user", "content": f"Translate if needed: {topic}"}
                    ],
                    max_tokens=100
                )
                
                translated = response.choices[0].message.content.strip()
                if translated.lower() != topic.lower():
                    search_queries.append(translated)
                    logger.info(f"ğŸ”¤ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : {translated}")
                    
                    # Also add more specific variants
                    search_queries.append(f"{translated} guide")
                    search_queries.append(f"{translated} tutorial")
                
            except Exception as e:
                # ç¿»è¨³ã«å¤±æ•—ã—ã¦ã‚‚ã€å‡¦ç†ã‚’åœæ­¢ã›ãšã«ä»£æ›¿ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
                logger.warning(f"âš ï¸ æ¤œç´¢ã‚¯ã‚¨ãƒªå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚ˆãã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã®å ´åˆã¯ã€æ‰‹å‹•ã§ç¿»è¨³ã‚’è©¦ã¿ã‚‹
                if "é‡å­" in topic:
                    search_queries.append("Quantum computing")
                    search_queries.append("Quantum physics")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Quantum computing")
                elif "äººå·¥çŸ¥èƒ½" in topic or "AI" in topic.upper():
                    search_queries.append("Artificial Intelligence")
                    search_queries.append("AI technology")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Artificial Intelligence")
                elif "æ©Ÿæ¢°å­¦ç¿’" in topic:
                    search_queries.append("Machine Learning")
                    search_queries.append("ML algorithms")
                    logger.info("ğŸ”¤ ä»£æ›¿ç¿»è¨³ã‚¯ã‚¨ãƒªã‚’è¿½åŠ : Machine Learning")
                # ãã®ä»–ã®ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯
                else:
                    # è‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                    search_queries.append("guide tutorial")
                    search_queries.append("introduction overview")
                    logger.info("ğŸ”¤ ä¸€èˆ¬çš„ãªè‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        
        # First layer: Primary search - try multiple queries until we get results
        all_primary_results = []
        
        for query in search_queries:
            logger.info(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ: {query}")
            primary_results = self.search_web(query, num_results=primary_results_count, depth=depth)
            
            if primary_results:
                all_primary_results.extend(primary_results)
                
                # For "low" depth, check if we have enough unique sources
                if depth == "low":
                    # Count unique domains in our results
                    unique_domains = set()
                    for result in all_primary_results:
                        try:
                            from urllib.parse import urlparse
                            domain = urlparse(result.url).netloc
                            unique_domains.add(domain)
                        except:
                            # If URL parsing fails, count the whole URL
                            unique_domains.add(result.url)
                    
                    if len(unique_domains) >= required_sources:
                        logger.info(f"âœ… '{depth}' æ·±åº¦ã§å¿…è¦ãª {required_sources} ä»¶ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºä¿")
                        break
                # For other depths, use the original logic
                elif len(all_primary_results) >= primary_results_count // 2:
                    logger.info("âœ… ååˆ†ãªæ¤œç´¢çµæœã‚’å–å¾—")
                    break  # We have enough results, stop trying more queries
        
        # Deduplicate results
        seen_urls = set()
        unique_primary_results = []
        for result in all_primary_results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_primary_results.append(result)
        
        # Evaluate credibility of primary results
        primary_results = self.evaluate_credibility(unique_primary_results)
        
        # Cluster to reduce redundancy
        primary_results = self.cluster_results(primary_results)
        research_result.primary_results = primary_results
        
        if layers >= 2 and primary_results:
            # Extract knowledge gaps for secondary searches
            logger.info("ğŸ”¬ è¿½åŠ èª¿æŸ»é …ç›®ã‚’ç‰¹å®šä¸­...")
            knowledge_gaps = self.extract_knowledge_gaps(primary_results, topic, depth)
            research_result.knowledge_gaps = knowledge_gaps
            
            # Second layer: Search for each knowledge gap
            logger.info("ğŸ” è©³ç´°æƒ…å ±ã®æ¤œç´¢ã‚’é–‹å§‹...")
            secondary_results = []
            
            # Use only a subset of knowledge gaps based on depth
            for gap in knowledge_gaps[:secondary_search_count]:
                # Create a more specific search query
                for base_query in search_queries[:1]:  # Use only the first query as base to limit requests
                    specific_query = f"{base_query} {gap}"
                    logger.info(f"ğŸ” è©³ç´°æ¤œç´¢: {specific_query}")
                gap_results = self.search_web(specific_query, num_results=5, depth=secondary_search_depth)
                
                # Evaluate and add to secondary results
                if gap_results:
                    gap_results = self.evaluate_credibility(gap_results)
                    secondary_results.extend(gap_results)
                    logger.info(f"âœ… è©³ç´°æƒ…å ±ã‚’å–å¾—: {len(gap_results)} ä»¶")
                    break  # If we got results, don't try other base queries
            
            # Cluster secondary results
            if secondary_results:
                secondary_results = self.cluster_results(secondary_results)
                research_result.secondary_results = secondary_results
        
        # If we still have no results (primary or secondary), generate basic knowledge
        if not research_result.primary_results and not research_result.secondary_results:
            logger.warning("âš ï¸ æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬æƒ…å ±ã‚’ç”Ÿæˆã—ã¾ã™")
            synthetic_results = self._generate_synthetic_results(topic, 5)
            research_result.primary_results = synthetic_results
            
            # Also generate some knowledge gaps
            research_result.knowledge_gaps = [
                f"Fundamentals of {topic}",
                f"Applications of {topic}",
                f"Best practices for {topic}",
                f"Future developments in {topic}"
            ]
        
        # Generate a comprehensive summary
        logger.info("ğŸ“ èª¿æŸ»çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆä¸­...")
        research_result.summary = self.generate_summary(research_result)
        
        logger.info("âœ… ãƒˆãƒ”ãƒƒã‚¯èª¿æŸ»å®Œäº†")
        return research_result
    
    def generate_summary(self, research: ResearchResult) -> str:
        """Generate a comprehensive summary of the research findings."""
        logger.info("ğŸ“Š æƒ…å ±ã‚’è¦ç´„ä¸­...")
        
        # Check if we already have a good summary
        if research.summary and len(research.summary.split()) > 200:
            logger.info("âœ… æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            print("\nğŸ“‹ ãƒªã‚µãƒ¼ãƒã‚µãƒãƒªãƒ¼:")
            print(research.summary[:500] + "..." if len(research.summary) > 500 else research.summary)
            return research.summary
        
        # Strategy 1: Use OpenAI API to generate a summary from the search results
        summary = None
        try:
            # Collect all relevant snippets and titles for context
            research_context = []
            
            # Start with higher credibility primary results
            for result in sorted(research.primary_results, key=lambda x: x.credibility_score, reverse=True):
                research_context.append(f"Title: {result.title}")
                research_context.append(f"Snippet: {result.snippet}")
                if result.content:
                    truncated = (result.content[:500] + "...") if len(result.content) > 500 else result.content
                    research_context.append(f"Content: {truncated}")
                research_context.append("---")
            
            # Add some secondary results if needed
            if len(research_context) < 1000 and research.secondary_results:
                for result in sorted(research.secondary_results, key=lambda x: x.credibility_score, reverse=True)[:5]:
                    research_context.append(f"Title: {result.title}")
                    research_context.append(f"Snippet: {result.snippet}")
                    research_context.append("---")
            
            prompt = f"""
            Create a comprehensive summary about "{research.topic}" based on the following research findings:
            
            {"\n".join(research_context[:3500])}
            
            Your summary should:
            1. Present key concepts, facts and insights in a well-structured way
            2. Organize information logically with headings and subheadings
            3. Be educational and informative, suitable for a presentation slide deck
            4. Include the most important points from multiple sources
            5. Use markdown format with ## for headings and ### for subheadings
            
            Aim for a comprehensive summary of 500-800 words.
            """
            
            # Using a more robust model for summary generation
            response = client.chat.completions.create(
                model=self.model,  # Use the most capable model available
                messages=[
                    {"role": "system", "content": "You are an expert researcher who creates comprehensive summaries from multiple sources."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3  # Low temperature for more factual output
            )
            
            summary = response.choices[0].message.content.strip()
            
            if summary and len(summary.split()) > 50:
                logger.info("âœ… ã‚µãƒãƒªãƒ¼ä½œæˆå®Œäº†")
                print("\nğŸ“‹ ãƒªã‚µãƒ¼ãƒã‚µãƒãƒªãƒ¼:")
                print(summary[:500] + "..." if len(summary) > 500 else summary)
                return summary
            else:
                logger.warning("âš ï¸ ç”Ÿæˆã•ã‚ŒãŸã‚µãƒãƒªãƒ¼ãŒçŸ­ã™ãã¾ã™")
                
        except Exception as e:
            logger.error(f"âŒ ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (æˆ¦ç•¥ 1): {e}")
        
        # Strategy 2: Alternative approach with a simpler prompt
        try:
            # Using a more straightforward prompt
            alternative_prompt = f"""
            Topic: {research.topic}
            
            Please create a detailed educational summary on this topic using the following information:
            
            {research.primary_results[0].snippet if research.primary_results else ""}
            {research.primary_results[1].snippet if len(research.primary_results) > 1 else ""}
            {research.primary_results[2].snippet if len(research.primary_results) > 2 else ""}
            
            Format your response as a structured markdown document with:
            - ## Main headings
            - ### Subheadings
            - Paragraphs with clear explanations
            - Important concepts highlighted
            
            The summary should be comprehensive enough for a presentation.
            """
            
            response = client.chat.completions.create(
                model=self.search_model,  # Try with the search model if main model failed
                messages=[
                    {"role": "system", "content": "You are a knowledgeable educator creating clear and structured topic summaries."},
                    {"role": "user", "content": alternative_prompt}
                ],
                max_tokens=1500
            )
            
            summary = response.choices[0].message.content.strip()
            
            if summary and len(summary.split()) > 50:
                logger.info("âœ… ä»£æ›¿ã‚µãƒãƒªãƒ¼ä½œæˆå®Œäº† (æˆ¦ç•¥ 2)")
                print("\nğŸ“‹ ãƒªã‚µãƒ¼ãƒã‚µãƒãƒªãƒ¼:")
                print(summary[:500] + "..." if len(summary) > 500 else summary)
                return summary
                
        except Exception as e:
            logger.error(f"âŒ ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (æˆ¦ç•¥ 2): {e}")
        
        # Emergency fallback - generate a basic summary using titles and snippets
        try:
            # Generate a basic summary using the available information
            
            # Collect titles from primary results
            titles = [result.title for result in research.primary_results]
            unique_titles = list(set(titles))
            
            # Create a basic markdown structure
            basic_summary = f"# {research.topic}\n\n"
            
            # Add a basic definition if available
            if research.primary_results:
                basic_summary += f"## Definition\n{research.primary_results[0].snippet}\n\n"
            
            # Add key concepts based on titles
            basic_summary += "## Key Concepts\n\n"
            
            # Extract key topics from titles
            topics = set()
            for title in unique_titles[:5]:  # Use up to 5 titles
                words = title.split()
                if len(words) > 2:
                    topics.add(" ".join(words[:3]))  # Use first 3 words of each title
            
            # Add each topic as a subheading
            for i, topic in enumerate(list(topics)[:3]):  # Use up to 3 topics
                basic_summary += f"### {i+1}. {topic}\n"
                
                # Find a relevant snippet
                for result in research.primary_results:
                    if any(word.lower() in result.snippet.lower() for word in topic.split()):
                        basic_summary += f"{result.snippet}\n\n"
                        break
                else:
                    # If no relevant snippet found, use a generic placeholder
                    basic_summary += f"Information about {topic} and its applications.\n\n"
            
            # Add a conclusion
            basic_summary += f"## Summary\n{research.topic} encompasses various important concepts and applications as outlined above."
            
            logger.info("âœ… åŸºæœ¬æƒ…å ±ã‚’ä½¿ç”¨ã—ãŸã‚µãƒãƒªãƒ¼ä½œæˆå®Œäº†")
            print("\nğŸ“‹ ãƒªã‚µãƒ¼ãƒã‚µãƒãƒªãƒ¼:")
            print(basic_summary[:500] + "..." if len(basic_summary) > 500 else basic_summary)
            return basic_summary
            
        except Exception as e:
            logger.error(f"âŒ ç·Šæ€¥ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            
            # Ultimate fallback - just concatenate titles and snippets
            fallback = f"# {research.topic}\n\n"
            fallback += "## Overview\n"
            if research.primary_results:
                for i, result in enumerate(research.primary_results[:3]):
                    fallback += f"- {result.title}: {result.snippet}\n"
            else:
                fallback += f"Information about {research.topic}."
            
            return fallback

    def _search_with_alternative_provider(self, query: str, num_results: int = 5) -> List[SearchResult]:
        """Use alternative search providers when OpenAI API is unavailable."""
        logger.info(f"ğŸ” ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ¤œç´¢ä¸­: {query}")
        
        try:
            # Try SerpAPI if key is available
            if self.serp_api_key:
                return self._search_with_serpapi(query, num_results)
            
            # Add additional alternative search providers here
            # For example, you could implement:
            # - DuckDuckGo API (free)
            # - Bing API
            # - Custom web scraping solution
            
            # If no alternative providers are available
            logger.warning("âš ï¸ ä»£æ›¿æ¤œç´¢ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        except Exception as e:
            logger.error(f"âŒ ä»£æ›¿æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
    def _search_with_serpapi(self, query: str, num_results: int = 5) -> List[SearchResult]:
        """Perform search using SerpAPI."""
        try:
            import requests
            
            # Set up SerpAPI request
            params = {
                "engine": "google",
                "q": query,
                "api_key": self.serp_api_key,
                "num": num_results
            }
            
            # Make request to SerpAPI
            response = requests.get(
                "https://serpapi.com/search", 
                params=params
            )
            
            # Check if request was successful
            if response.status_code == 200:
                data = response.json()
                results = []
                
                # Extract organic results
                if "organic_results" in data:
                    for result in data["organic_results"][:num_results]:
                        search_result = SearchResult(
                            url=result.get("link", ""),
                            title=result.get("title", ""),
                            snippet=result.get("snippet", ""),
                            source_type="serpapi"
                        )
                        results.append(search_result)
                
                if results:
                    logger.info(f"âœ… SerpAPIã‹ã‚‰ {len(results)} ä»¶ã®çµæœã‚’å–å¾—")
                    return results
            
            logger.warning(f"âš ï¸ SerpAPIæ¤œç´¢ãŒçµæœã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code})")
            return []
            
        except Exception as e:
            logger.error(f"âŒ SerpAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

# Convenience function for direct module use
def search_deep(topic: str, depth: str = "medium") -> ResearchResult:
    """
    Public API: Perform a deep search on a topic with the default ResearchAgent.
    
    Args:
        topic: The topic to research
        depth: Search depth - "low", "medium", or "high"
    """
    agent = ResearchAgent()
    return agent.search_deep(topic, depth)

def search_basic(topic: str, num_results: int = 5) -> List[Dict[str, str]]:
    """
    Perform a basic search on a topic and return simplified results
    
    Args:
        topic: The topic to search for
        num_results: Number of results to return
        
    Returns:
        List of simplified search results as dictionaries
    """
    logger.info(f"ğŸ” åŸºæœ¬æ¤œç´¢ã‚’å®Ÿè¡Œ: {topic} (çµæœæ•°: {num_results})")
    
    try:
        # Create a research agent
        agent = ResearchAgent()
        
        # Perform a simplified search
        search_results = agent.search_web(topic, num_results=num_results, depth="low")
        
        # Convert to simplified dictionary format and enrich with real titles if needed
        simplified_results = []
        
        for result in search_results:
            # Clean the URL
            url = result.url.strip()
            
            # Extract real title if it's in markdown format
            title = result.title.strip()
            markdown_title_pattern = r'\[(.*?)\]'
            markdown_title_match = re.search(markdown_title_pattern, title)
            if markdown_title_match:
                title = markdown_title_match.group(1).strip()
            else:
                # If not in markdown, clean up any other artifacts
                title = re.sub(r'\*\*|\*|[-â€¢]|\bURL:|\bURL\b', '', title).strip()
            
            # Check if the title is just a domain name
            if title.endswith('.jp') or title.endswith('.com') or title.endswith('.org') or title.endswith('.net'):
                # Try to get a better title from the URL domain parts
                domain_parts = url.split('://')[-1].split('/')[0].split('.')
                if len(domain_parts) >= 2:
                    site_name = domain_parts[-2].capitalize()
                    title = f"{site_name} - {topic}ã«é–¢ã™ã‚‹æƒ…å ±"
            
            # Clean up the summary
            summary = result.snippet.strip() if result.snippet else ""
            
            # If summary is just a URL remnant or empty, provide a generic summary
            if not summary or summary.startswith('ook/') or summary.endswith('ai))"') or summary.endswith('ai))'):
                summary = f"{topic}ã«é–¢ã™ã‚‹æƒ…å ± - {title}"
            
            simplified_results.append({
                "title": title,
                "source": url,
                "summary": summary
            })
        
        logger.info(f"âœ… åŸºæœ¬æ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸ: {len(simplified_results)} ä»¶ã®çµæœ")
        return simplified_results
        
    except Exception as e:
        logger.error(f"âŒ åŸºæœ¬æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # Return a minimal result set to avoid breaking the pipeline
        dummy_results = []
        for i in range(min(num_results, 3)):
            dummy_results.append({
                "title": f"{topic} - æƒ…å ± {i+1}",
                "source": "https://example.com",
                "summary": f"{topic}ã«é–¢ã™ã‚‹æƒ…å ±ã§ã™ã€‚APIã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¤œç´¢ãŒã§ããªã„ãŸã‚ã€é™å®šçš„ãªæƒ…å ±ã®ã¿æä¾›ã—ã¦ã„ã¾ã™ã€‚"
            })
        return dummy_results 